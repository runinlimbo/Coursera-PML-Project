---
title: "Practical Machine Learning Project: *Predicting* Bicep Dumbell Lifting Technique"
author: "Jeremy Sidwell"
date: "11/6/2018"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Libaries applied in project:
library(AppliedPredictiveModeling)
library(caret)
library(ElemStatLearn)
library(pgmm)
library(rpart)
#install.packages("gbm")
library(gbm)
library(lubridate)
#install.packages("forecast")
library(forecast)
library(e1071)
library(randomForest)
```


```{r echo=FALSE, results="hide"}
#Load Test and Training Data
# Data has been downloaded from course website and saved to the following locations:
training = read.csv("~/Documents/DataScience/Course 8/Programming Assignment/DATA/pml-training.csv")
testing  = read.csv("~/Documents/DataScience/Course 8/Programming Assignment/DATA/pml-testing.csv")

sensorData = grep(pattern = "belt|arm|dumbbell|forearm|classe",names(training))
head(sensorData)
training = training[,c(sensorData)]
train.na<-apply(training,2,function(x) gsub("^$|^ $", NA, x))
training.clean = training[,which(colSums(!is.na(train.na))>14716)] #75% of 19622
table(complete.cases(training.clean))

# Model Build
#Fit 1) RF, 2) Boosted "gmb", 3) LDA.  
set.seed(12345)
#Ensure Classe Variable is a factor
inTrain <- createDataPartition(training.clean$classe, p = 3/4)[[1]]
training.train = training.clean[ inTrain,]
training.validate = training.clean[-inTrain,]

control <- trainControl(method = "cv", number = 5)
modFitRF <- train(classe~., data=training.train, method="rf", trControl = control)
modFitGBM <- train(classe~.,data=training.train, method="gbm", trControl = control)
modFitLDA <- train(classe~.,data=training.train, method="lda", trControl = control)

# Model Testing on Validation Set
predRF <- predict(modFitRF,training.validate)
predGBM<-predict(modFitGBM,training.validate)
predLDA <- predict(modFitLDA,training.validate)
cmRF <- confusionMatrix(predRF, training.validate$classe)
cmGBM <- confusionMatrix(predGBM, training.validate$classe)
cmLDA <- confusionMatrix(predLDA, training.validate$classe)

overallRF <- cmRF$overall
overallGBM <- cmGBM$overall
overallLDA <- cmLDA$overall

overallRF
Model.Names <- names(overallRF[1:6])
Model.Names <- c("Model_Type",Model.Names)
Model.Names

# Create Percent function to format output for write-up
percent <- function(x, digits = 2, format = "f", ...) {
  paste0(formatC(100 * x, format = format, digits = digits, ...), "%")
}
Model_Type <- c('RF','GBM','LDA')
Model.Output <- cbind(Model_Type,rbind(percent(overallRF[1:6]),percent(overallGBM[1:6]),percent(overallLDA[1:6])))
colnames(Model.Output)<-Model.Names


#Best Fit Model: Random Forest
# In Sample Error
predTrainRF <- predict(modFitRF, training.train)
Accuracy.Train <- sum(predTrainRF == training.train$classe)/length(predTrainRF)
InSampleError <- (1-Accuracy.Train)*100
round(InSampleError, digits= 2) # In Sample Error Estimate: 0.0%

# Out of Sample Error appyling the best fit model:
# Accuracy of the model on Test Data
Accuracy.Validate <- round(100*sum(predRF == training.validate$classe)/length(predRF),digits=2)
# Out of Sample Error
OutOfSampleError <- round((100-Accuracy.Validate),digits = 2)

# Predict on the Test Set
testRF <- predict(modFitRF,testing)
```

## Executive Summary


This project explores various models that predict the manner in which test subjects perform a dumbbell bicep curl exercise.  Three models were tested on the provided data.  The best performing model of those reviewed, based on level of accuracy, was the random forest model (**`r Accuracy.Validate`% accurate on the validation set**).  The chosen model was then run on the 20 test cases to predict the technique used in each case.

## Background

The goal of this project is to predict the manner in which test subjects perform a dumbbell bicep curl exercise.  Data for this project is sourced from the Qualitative Activity Recognition of Weight Lifting Exercises study, conducted by Velloso E.; Bulling, A.; Gellersen, H.; Ugulino, W; and Fulks, H.  The data was collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.  Participants were male with limited weight lifting experience, between the ages of 20-28.  Participants were asked to perform dumbbell lifts correctly and incorrectly in 5 different ways under trainer supervision. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).  The course website provides a link to a training set (19,622 rows, 160 variables), and a testing set (20 samples, 160 variables).  

## Dataset

For purposes of this project, I have assumed data collection is complete and accurate; however, certain variables were omitted given missing values or irrelevant to predicting the outcome.  Variables have been limited to the target variable (classe) and sensor data (belt, forearm, arm, or dumbbell); and omits all fields where more than 25% of entries are empty :

```{r, eval = FALSE, echo = TRUE, results="hide"}
sensorData = grep(pattern = "belt|arm|dumbbell|forearm|classe",names(training))
training = training[,c(sensorData)]
train.na<-apply(training,2,function(x) gsub("^$|^ $", NA, x))
training.clean = training[,which(colSums(!is.na(train.na))>14716)] #75% of 19622
```

I also tested that there are an appropriate number of entries for each classe.  Results show 5,580 entries in Classe A, between 3k and 4k in Classe B-E.  The final training data set includes 19,622 observations and 53 variables.  Correlations amongst the remaining variables (excluding the outcome factor variable, classe) were also reviewed, which showed the majority have low to no (near zero) correlation. 

##  Model Build

Given that it is common to observe noise in sensor data, as referenced in the cited paper, I  have followed the authors' recommendation and applied a Random Forest (RF) approach to predict the manner in which test subjects perform a dumbbell bicep curl exercise.  I have also tested model fit using generalized boosted trees (GBM) and linear discriminent analysis (LDA).  In each case, I predict the outcome using a five-fold cross validation and then compare models for the best fit (optimal accuracy).  The five-fold cross validation randomly partitions the training sample into five equal sized subsamples.  The cross-validation is then repeated five times.  I selected k=5 versus a larger k, such as 10, to minimize runtime.  Increasing k until k = 10, often improves both the bias and variance; however, the time trade-off led me to select k = 5.   Treating the testing dataset as the validation data, I further break down the provided training dataset to 75% as training data and 25% as validation data:

```{r, eval = FALSE, echo = TRUE, results="hide"}
inTrain <- createDataPartition(training.clean$classe, p = 3/4)[[1]]
training.train = training.clean[ inTrain,]
training.validate = training.clean[-inTrain,]
control <- trainControl(method = "cv", number = 5)
```

I use the following code to execute each model build:

```{r, eval = FALSE, echo = TRUE, results="hide"}
modFitRF <- train(classe~., data=training.train, method="rf", trControl = control) #Random Forest
modFitGBM <- train(classe~., data=training.train, method="gbm", trControl = control) #Generalized Boosted Trees
modFitLDA <- train(classe~., data=training.train, method="lda", trControl = control) #Linear Discriminant Analysis
```


## Model Accuracy, and Out of Sample Error
Testing each model on the 25% validation dataset, I reviewed the accuracy to determine the best model and the Out of Same Error Estimate of the best model.
```{r, eval = FALSE, echo = TRUE, results="hide"}
# Model Testing
predRF <- predict(modFitRF,training.validate)
predGBM<-predict(modFitGBM,training.validate)
predLDA <- predict(modFitLDA,training.validate)
# Confusion Matrix
cmRF <- confusionMatrix(predRF, training.validate$classe)
cmGBM <- confusionMatrix(predGBM, training.validate$classe)
cmLDA <- confusionMatrix(predLDA, training.validate$classe)
```

Based on overall accuracy results, the Random Forest Model holds the greatest accuracy (**`r Accuracy.Validate`%**) on the validation dataset.  The **out of sample error estimate** is equivalent to 1 minus the accuracy of the RF model on the validation set, which is equal to **`r OutOfSampleError`%**.  Overall Accuracy Results for each model is below: 
```{r include = FALSE}
library(knitr)
library(kableExtra)
```
```{r}
kable(Model.Output) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


## Test Set

Below, I have used my Final RF prediction model to predict the 20 different test cases.  

```{r, echo = TRUE}
 testRF <- predict(modFitRF, testing)
 testRF
```

This test set may be used to asses the performance of the fully-trained model. 

